{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vdKUNmZQuzH"
   },
   "source": [
    "## Components in this RAG project\n",
    "\n",
    "1. **External Data Source**: `University-of-North-Georgia-Employee-Handbook.pdf`. The text data from the external source is parsed and converted to a numerical representation known as embeddings. The relevant embeddings are then retrieved and integrated with the internal representation of an LLM.\n",
    "2. **Embedding Model**: An embedding model is used to convert the text data to numerical embeddings, also known as vectors. The SentenceTransformer `sentence-transformers/all-MiniLM-L6-v2` is used in this project.\n",
    "3. **Vector Document Store**: The embeddings constructed from the external data sources are stored in a vector database to facilitate efficient lookup and similarity search. `InMemoryDocumentStore` is used in this project.\n",
    "4. **Orchestration Pipeline**: An end-to-end framework is required to extract data and embeddings, seamlessly integrate with the vector document store, apply semantic search on embeddings, and generate responses for scalable, production-ready deployment of RAG. This framework essentially consists of a pipeline that can preprocess the external data, chunk them into sizeable units, convert the text data to numerical embeddings, retrieve the embeddings per user query, and facilitate the integration of the retrieved embeddings with generative AI logic of the LLM. `Haystack` is used in this project.\n",
    "5. **Large Language Model**: An LLM is at the heart of the RAG framework to power the generative AI mechanism. `GPT-4o-mini` is used in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyKfAuvMQuzK"
   },
   "source": [
    "## Architecture of Haystack AI\n",
    "Haystack’s architecture is designed to be modular and flexible, allowing developers to build custom pipelines tailored to their specific needs. It is made of two main architectures: components and pipelines.\n",
    "\n",
    "Here’s a brief overview of some of its key components:\n",
    "* **Document Store**: The storage system for documents. It can be in-memory or persistent.\n",
    "* **Retriever**: Finds relevant documents from the document store based on a query.\n",
    "* **Generator**: Extracts or generates answers from the retrieved documents.\n",
    "* **Pipeline**: Connects all components in a sequence to process the input query and generate the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvgFshWlQuzK"
   },
   "source": [
    "Use pip to install the Haystack 2.x with \"haystack-ai\". Do not use \"farm-haystack\" which is for Haystack 1.x version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cLNqhyClQuzL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install haystack-ai\n",
    "# pip install \"sentence-transformers>=2.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:11:00.898150Z",
     "iopub.status.busy": "2024-12-06T21:11:00.897795Z",
     "iopub.status.idle": "2024-12-06T21:11:00.904075Z",
     "shell.execute_reply": "2024-12-06T21:11:00.902555Z",
     "shell.execute_reply.started": "2024-12-06T21:11:00.898115Z"
    },
    "id": "F6Mu1pO5QuzM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# file_name = '/kaggle/input/employee-handbook/University-of-North-Georgia-Employee-Handbook.pdf'\n",
    "file_name = 'documents/University-of-North-Georgia-Employee-Handbook.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC_8JPTuQuzM"
   },
   "source": [
    "## Two Pipelines\n",
    "\n",
    "1. Indexing Pipeline: preprocess the external data from a pdf file, chunk them into sizeable units, convert the text data to numerical embeddings and save into a vector document store.\n",
    "2.\n",
    "Query Pipeline: retrieve the embeddings from user question, retrieve relevant documents from the document store, construct a prompt and use a language model to generate an answer.\n",
    "\n",
    "![image.png](attachment:d423d012-d802-4e6f-b010-ee257e3cf76e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bk96BGtQuzM"
   },
   "source": [
    "We need to import all necessary libraries from Haystack framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:11:00.906312Z",
     "iopub.status.busy": "2024-12-06T21:11:00.905778Z",
     "iopub.status.idle": "2024-12-06T21:11:28.769334Z",
     "shell.execute_reply": "2024-12-06T21:11:28.768190Z",
     "shell.execute_reply.started": "2024-12-06T21:11:00.906260Z"
    },
    "id": "ujyJ1H5jQuzM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder, SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pypdf\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM8cRdUsQuzN"
   },
   "source": [
    "## Building Indexing Pipeline\n",
    "\n",
    "Our indexing pipeline will convert the PDF file into a Haystack Document using PyPDFToDocument and preprocess it by cleaning and splitting it into chunks before storing them in InMemoryDocumentStore.\n",
    "\n",
    "Use SentenceTransformersTextEmbedder to embed PDF document and store in Document Store.\n",
    "\n",
    "InMemoryDocumentStore is a pure Python data structures stored in memory. It is ideal for creating quick prototypes with small datasets. It doesn’t require any special setup, and it can be used right away without installing additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EuBirJ55QuzN",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [00:05<00:00,  1.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 257}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "## Add components to the pipeline\n",
    "indexing_pipeline.add_component(\"converter\", PyPDFToDocument())\n",
    "indexing_pipeline.add_component(\"cleaner\", DocumentCleaner(\n",
    "    remove_empty_lines=True, remove_extra_whitespaces=True, remove_repeated_substrings=True))\n",
    "indexing_pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2))\n",
    "indexing_pipeline.add_component(\"embedder\", SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "\n",
    "## Connect the components to each other\n",
    "indexing_pipeline.connect(\"converter\", \"cleaner\")\n",
    "indexing_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
    "\n",
    "## Run the pipeline with the files you want to index.\n",
    "result = indexing_pipeline.run({\"converter\": {\"sources\": [Path(file_name)]}}) # use Path()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGOcwpeCQuzN"
   },
   "source": [
    "## Building Query Pipeline\n",
    "\n",
    "A query pipeline is used to receive a query from the user and produce a result. It has access to a DocumentStore which stores a set of documents. A common query pipeline is designed to return a result based on the documents stored in the DocumentStore it has access to.\n",
    "\n",
    "\n",
    "In this pipeline, we’ll use `InMemoryEmbeddingRetriever` to retrieve relevant information from the `InMemoryDocumentStore` and a GPT model `GPT-4o-mini` to generate answers with `OpenAIGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:13:56.908087Z",
     "iopub.status.busy": "2024-12-06T21:13:56.907717Z",
     "iopub.status.idle": "2024-12-06T21:13:57.098464Z",
     "shell.execute_reply": "2024-12-06T21:13:57.097324Z",
     "shell.execute_reply.started": "2024-12-06T21:13:56.908055Z"
    },
    "id": "EMnxTFsEQuzN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# user_secrets = UserSecretsClient()\n",
    "# openai_api_key = Secret.from_token(user_secrets.get_secret(\"OPEN_API_KEY2\"))\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "openai_api_key = Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzVbTvfIQuzO"
   },
   "source": [
    "The **prompt template** specifies how the context (retrieved documents) and the question should be formatted. For different business contexts, the prompt templates may vary based on the required level of detail or the desired tone of the answer.\n",
    "- `documents` is passed from retriever\n",
    "- `question` is passed as a param in pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:14:03.108324Z",
     "iopub.status.busy": "2024-12-06T21:14:03.107960Z",
     "iopub.status.idle": "2024-12-06T21:14:03.113750Z",
     "shell.execute_reply": "2024-12-06T21:14:03.112612Z",
     "shell.execute_reply.started": "2024-12-06T21:14:03.108292Z"
    },
    "id": "jiaJ-V1FQuzO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    Synthesize a comprehensive answer from the following text for the given question.\n",
    "    Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
    "    Your answer should be in your own words and be no longer than 40 words.\n",
    "    Context:\n",
    "    {% for document in documents %}\n",
    "        {{ document.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    Question: {{question}}\n",
    "    Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTjuV68vQuzO"
   },
   "source": [
    "Use `SentenceTransformersTextEmbedder` to embed the question before sending it to a `PromptBuilder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:14:03.115742Z",
     "iopub.status.busy": "2024-12-06T21:14:03.115384Z",
     "iopub.status.idle": "2024-12-06T21:14:03.152906Z",
     "shell.execute_reply": "2024-12-06T21:14:03.151613Z",
     "shell.execute_reply.started": "2024-12-06T21:14:03.115703Z"
    },
    "id": "7Im_Sd4FQuzO",
    "outputId": "1349b048-5a4a-40dd-9d0d-4b563a45a8c4",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000001B40C59FBB0>\n",
       "🚅 Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_pipeline = Pipeline()\n",
    "\n",
    "# Add components to your pipeline\n",
    "query_pipeline.add_component(\"text_embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "query_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=2))\n",
    "query_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "query_pipeline.add_component(\"llm\", OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=openai_api_key,\n",
    "    generation_kwargs={\"temperature\": 0}\n",
    "))\n",
    "\n",
    "# Now, connect the components to each other\n",
    "# connect(input, output)\n",
    "query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "query_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "query_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:14:03.154553Z",
     "iopub.status.busy": "2024-12-06T21:14:03.154126Z",
     "iopub.status.idle": "2024-12-06T21:14:03.159894Z",
     "shell.execute_reply": "2024-12-06T21:14:03.158746Z",
     "shell.execute_reply.started": "2024-12-06T21:14:03.154503Z"
    },
    "id": "H7P8_8H8QuzP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question = \"Is orientation mandatory for a student assistant?\"\n",
    "question = \"What are the consequences for non-exempt employees who fail to record their time worked accurately in the OneUSG Connect system?\"\n",
    "question = \"How are insurance premiums deducted from the paychecks of bi-weekly paid employees, and how does this differ for monthly staff?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9026d234fb904ff88de474a4ea8a86a2"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-06T21:14:03.162089Z",
     "iopub.status.busy": "2024-12-06T21:14:03.161751Z",
     "iopub.status.idle": "2024-12-06T21:14:04.152015Z",
     "shell.execute_reply": "2024-12-06T21:14:04.150831Z",
     "shell.execute_reply.started": "2024-12-06T21:14:03.162055Z"
    },
    "id": "gFgXCi1kQuzP",
    "outputId": "01fe7385-b43f-4103-db0b-150c6db6b304",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Answer: Bi-weekly employees have medical insurance premiums deducted from their first two paychecks monthly on a pre-tax basis, while monthly staff have premiums deducted on the last working day of the month, also on a pre-tax basis.\n"
     ]
    }
   ],
   "source": [
    "response = query_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\":{\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question}\n",
    "    }\n",
    ")\n",
    "print(f'🤖 Answer: {response[\"llm\"][\"replies\"][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LpqH2K7QuzQ"
   },
   "source": [
    "## Relevant Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "cc4508d31bdd422bbb7e632b7005745b"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-06T21:19:49.494333Z",
     "iopub.status.busy": "2024-12-06T21:19:49.493877Z",
     "iopub.status.idle": "2024-12-06T21:19:49.558229Z",
     "shell.execute_reply": "2024-12-06T21:19:49.556376Z",
     "shell.execute_reply.started": "2024-12-06T21:19:49.494292Z"
    },
    "id": "Iz4DUceGQuzQ",
    "outputId": "11d1c7cd-c191-4c6a-dcd1-b588489a2b37",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑 Question: How are insurance premiums deducted from the paychecks of bi-weekly paid employees, and how does this differ for monthly staff?\n",
      "🤖 Relevant Documents:\n",
      " Falsification of or failure to record time may result in failure to get paid and/or disciplinary action including termination. ALL TIME RECORDS MUST BE APPROVED BY BOTH THE EMPLOYEE AND AN AUTHORIZED REVIEWING AUTHORITY. 2.11.2 Paychecks Non-exempt employees are paid bi-weekly, with payments made every other Friday. Any pay adjustments not reporting to Payroll Services in time for processing in the current payroll will be reflected on the next paycheck. Insurance premiums are deducted out of the first two paychecks of each month but will be adjusted by payroll as needed to collect any amounts due.\fPage | 50 Monthly staff are compensated on the last working day of each month that includes premium deductions as needed. 2.11.\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval_pipeline = Pipeline()\n",
    "\n",
    "retrieval_pipeline.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "retrieval_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store, top_k=1))\n",
    "retrieval_pipeline.connect(\"embedder\", \"retriever\")\n",
    "\n",
    "retrieval_result = retrieval_pipeline.run({\"embedder\": {\"text\": question}})\n",
    "\n",
    "print(f\"🧑 Question: {question}\")\n",
    "print(f\"🤖 Relevant Documents:\")\n",
    "for doc in retrieval_result[\"retriever\"][\"documents\"]:\n",
    "    print(doc.content)\n",
    "    print(\"-\"*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSMa0RYRQuzQ"
   },
   "source": [
    "## References\n",
    "1. https://medium.com/deepset-ai/pdf-based-question-answering-with-amazon-bedrock-and-haystack-dc234003ffd8\n",
    "2. https://medium.com/@researchgraph/a-technical-guide-to-haystack-ai-e1a95bae4d96\n",
    "3. https://www.e2enetworks.com/blog/rag-implementation-using-mistral-7b-haystack-and-weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVzel3S_QuzQ"
   },
   "source": [
    "## Have Fun with \"The Godfather\"\n",
    "\n",
    "We can reuse the same 2 pipelines created above to digest “*Mario_Puzo-The_Godfather.pdf*”. This book is 6 times larger than the handbook, so please expect the preprocessing will take longer.\n",
    "\n",
    "Take a look at some example questions and answers below. I found out that the RAG technique is not very suitable for Q&A with stories, as the answers often require **inference from multiple parts** of the narrative. In contrast, answers for informational documents, such as handbooks or guides, typically come from just one or two specific sections of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "017c9f51afef4aca9781ada90646067d"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-06T21:20:32.811089Z",
     "iopub.status.busy": "2024-12-06T21:20:32.810692Z",
     "iopub.status.idle": "2024-12-06T21:21:51.180788Z",
     "shell.execute_reply": "2024-12-06T21:21:51.179634Z",
     "shell.execute_reply.started": "2024-12-06T21:20:32.811053Z"
    },
    "id": "rIao3yRxQuzQ",
    "outputId": "e53fac7b-7493-4458-b87e-441da31c516c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 49/49 [00:31<00:00,  1.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 1559}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_name = '/kaggle/input/pdf-books/pdf/Mario_Puzo-The_Godfather.pdf'\n",
    "file_name = 'documents/Mario_Puzo-The_Godfather.pdf'\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "result = indexing_pipeline.run({\"converter\": {\"sources\": [Path(file_name)]}})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "c80058114b4d480080caa39d67a7cbef"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-06T21:21:51.213396Z",
     "iopub.status.busy": "2024-12-06T21:21:51.213061Z",
     "iopub.status.idle": "2024-12-06T21:21:52.060102Z",
     "shell.execute_reply": "2024-12-06T21:21:52.058726Z",
     "shell.execute_reply.started": "2024-12-06T21:21:51.213361Z"
    },
    "id": "oaP0lzMTQuzQ",
    "outputId": "a79bd024-09f0-4999-a597-a9321e18fe57",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Answer: The men writing down license plate numbers outside Connie's wedding are assigned by the Don to monitor Carlo Rizzi's bookmaking operation, reporting to Tom Hagen.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who are Don Corleone's children?\"\n",
    "question = \"Why has Don Corleone been shot?\"\n",
    "question = \"What is unusual about Tom Hagen being chosen for Don Corleone's consigliori?\" # In The Godfather\n",
    "question = \"Who are the men writing down license plate numbers outside Connie's wedding?\"\n",
    "\n",
    "response = query_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\":{\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question}\n",
    "    }\n",
    ")\n",
    "print(f'🤖 Answer: {response[\"llm\"][\"replies\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDockQtqQuzQ",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
