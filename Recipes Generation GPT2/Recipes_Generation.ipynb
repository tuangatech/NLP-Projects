{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:43:33.236513700Z",
     "start_time": "2024-03-08T16:43:33.132320400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device {device}\")\n",
    "\n",
    "model_name = \"gpt2-medium\"  # options: ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "model_save_path = './model'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:43:33.330716300Z",
     "start_time": "2024-03-08T16:43:33.142326500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quick Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "configuration = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "input_sequence = \"beef, salt, pepper\"\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "\n",
    "model = model.to(device)\n",
    "#combine both sampling techniques\n",
    "sample_outputs = model.generate(\n",
    "                              input_ids.to(device),\n",
    "                              do_sample = True,\n",
    "                              max_length = 120,\n",
    "                              top_k = 50,\n",
    "                              top_p = 0.85,\n",
    "                              num_return_sequences = 3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('  ---')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Data Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RecipeId', 'name', 'ingredients', 'instructions']\n",
      "data shape (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "df_recipes = pd.read_csv('recipes_1000.csv')\n",
    "df_recipes.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df_recipes = df_recipes.iloc[:600]\n",
    "print(list(df_recipes.columns))\n",
    "print(f\"data shape {df_recipes.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:44:04.847279800Z",
     "start_time": "2024-03-08T16:44:04.795030300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% documents > 180 tokens: 20.4%\n",
      "Average document length: 133\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "doc_lengths = []\n",
    "\n",
    "for rec in df_recipes.itertuples():\n",
    "\n",
    "    # get rough token count distribution\n",
    "    tokens = nltk.word_tokenize(rec.ingredients + ' ' + rec.instructions)\n",
    "\n",
    "    doc_lengths.append(len(tokens))\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "# the max token length\n",
    "print(f\"% documents > 180 tokens: {round(len(doc_lengths[doc_lengths > 180]) / len(doc_lengths) * 100, 1)}%\")\n",
    "print(f\"Average document length: {int(np.average(doc_lengths))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:44:05.772979800Z",
     "start_time": "2024-03-08T16:44:04.925995600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "\"<|startoftext|>Ingredients: blueberries, granulated sugar, vanilla yogurt, lemon juice. Instructions: Toss 2 cups berries with sugar. Let stand for 45 minutes, stirring occasionally. Transfer berry-sugar mixture to food processor. Add yogurt and process until smooth. Strain through fine sieve. Pour into baking pan (or transfer to ice cream maker and process according to manufacturers' directions). Freeze uncovered until edges are solid but centre is soft.  Transfer to processor and blend until smooth again. Return to pan and freeze until edges are solid. Transfer to processor and blend until smooth again. Fold in remaining 2 cups of blueberries. Pour into plastic mold and freeze overnight. Let soften slightly to serve.<|endoftext|>\""
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def form_string(ingredient,instruction):\n",
    "    # s = f\"<|startoftext|>Ingredients:\\n{ingredient.strip()}\\n\\nInstructions:\\n{instruction.strip()}<|endoftext|>\"\n",
    "    s = f\"<|startoftext|>Ingredients: {ingredient.strip()}. \" \\\n",
    "        f\"Instructions: {instruction.strip()}<|endoftext|>\"\n",
    "    return s\n",
    "\n",
    "def extract_string(recipe):\n",
    "    str = recipe.replace('<|startoftext|>', '').replace('<|endoftext|>', '')\n",
    "    inst_pos = str.find('Instructions: ')\n",
    "    ingredients = str[len('Ingredients: '): inst_pos-1]\n",
    "    instructions = str[inst_pos+len('Instructions: '):]\n",
    "    return ingredients, instructions\n",
    "\n",
    "data = df_recipes.apply(lambda x:form_string(\n",
    "    x['ingredients'], x['instructions']), axis=1).to_list()\n",
    "data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T15:42:56.592693700Z",
     "start_time": "2024-03-09T15:42:56.566399200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPT2 Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name,\n",
    "                                              bos_token='<|startoftext|>',\n",
    "                                              eos_token='<|endoftext|>',\n",
    "                                              unk_token='<|unknown|>',\n",
    "                                              pad_token='<|pad|>'\n",
    "                                             )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:44:05.943900500Z",
     "start_time": "2024-03-08T16:44:05.804297700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_list = sorted(tokenizer.vocab.items(), key=lambda x:x[1])\n",
    "for i in range(5555, 5566):\n",
    "    print(vocab_list[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The beginning of sequence token <|startoftext|> has the id 50257\n",
      "The unknown token <|unknown|> has the id 50258\n",
      "The padding token <|pad|> has the id 50259\n"
     ]
    }
   ],
   "source": [
    "print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The beginning of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The unknown token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.unk_token_id), tokenizer.unk_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:44:06.092572200Z",
     "start_time": "2024-03-08T16:44:06.049673500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch Datasets & Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "# GPT2 is a large model. Increasing the batch size above 2 has lead to out of memory problems.\n",
    "batch_size = 2\n",
    "max_length = 180  # maximum sentence length\n",
    "\n",
    "# standard PyTorch approach of loading data in using a Dataset class.\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.origin_ingredients = []\n",
    "        self.origin_instructions = []\n",
    "\n",
    "        for recipe in data:\n",
    "            encodings = tokenizer.encode_plus(recipe,\n",
    "                                              truncation=True,\n",
    "                                              padding='max_length',\n",
    "                                              max_length=max_length,\n",
    "                                              return_tensors='pt'       # return PyTorch tensor\n",
    "                                             )\n",
    "            self.input_ids.append(torch.squeeze(encodings['input_ids'],0))\n",
    "            # attention_mask tells model not to incorporate these PAD tokens into its interpretation of the sentence\n",
    "            self.attn_masks.append(torch.squeeze(encodings['attention_mask'],0))\n",
    "            ingredients, instructions = extract_string(recipe)\n",
    "            self.origin_ingredients.append(ingredients)\n",
    "            self.origin_instructions.append(instructions)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx], self.origin_ingredients[idx], self.origin_instructions[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T15:45:05.331123Z",
     "start_time": "2024-03-09T15:45:05.315491Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  900 training samples\n",
      "  100 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = RecipeDataset(data, tokenizer)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T15:45:10.641146700Z",
     "start_time": "2024-03-09T15:45:09.638292400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"dataset size {dataset.__len__()}\")\n",
    "print(f\"dataset[0]: \\n  input_ids: {dataset[0][0]}\\n  attn_masks: {dataset[0][1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T15:45:20.157582400Z",
     "start_time": "2024-03-09T15:45:20.131054200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetune GPT2 Language Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50260. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape torch.Size([50257, 1024])\n",
      "Number of tokens: 50260\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config.from_pretrained(model_name, output_hidden_states=False)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "model = model.to(device)\n",
    "print(f\"Weight shape {model.transformer.wte.weight.shape}\")\n",
    "# this step is necessary because I've added some tokens (bos_token, etc.) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Number of tokens: {len(tokenizer)}\")\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:44:12.209527600Z",
     "start_time": "2024-03-08T16:44:06.778388200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50260, 1024])\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = model.transformer.wte.weight # Word Token Embeddings\n",
    "\n",
    "print(word_embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:44:12.308408400Z",
     "start_time": "2024-03-08T16:44:12.209527600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 1e2\n",
    "# The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation”\n",
    "epsilon = 1e-8\n",
    "# optim = Adam(model.parameters(), lr=5e-5)\n",
    "optim = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:44:12.324043600Z",
     "start_time": "2024-03-08T16:44:12.224536300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T16:44:12.324043600Z",
     "start_time": "2024-03-08T16:44:12.266947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def infer(prompt):\n",
    "    input = f\"<|startoftext|>Ingredients: {prompt.strip()}\"\n",
    "    input = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids      = input[\"input_ids\"]\n",
    "    attention_mask = input[\"attention_mask\"]\n",
    "\n",
    "    output = model.generate(input_ids.to(device),\n",
    "                            attention_mask=attention_mask.to(device),\n",
    "                            max_new_tokens=max_length,\n",
    "                            # temperature = 0.5,\n",
    "                            do_sample = True, top_k = 50, top_p = 0.85)\n",
    "                            # num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T14:08:08.624763900Z",
     "start_time": "2024-03-09T14:08:08.609132100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()  # `train` just changes the *mode* (train vs. eval), it doesn't *perform* the training.\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):     # step from enumerate() = number of batches\n",
    "\n",
    "        b_input_ids = batch[0].to(device)   # tokens (of multiple documents in a batch)\n",
    "        b_labels    = batch[0].to(device)\n",
    "        b_masks     = batch[1].to(device)   # mask of [1] for a real word, [0] for a pad\n",
    "\n",
    "        model.zero_grad()\n",
    "        # loss = model(X.to(device), attention_mask=a.to(device), labels=X.to(device)).loss\n",
    "        outputs = model(  input_ids = b_input_ids,\n",
    "                          labels = b_labels,\n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids = None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            sample_output = infer(\"eggs, flour, butter, sugar\")\n",
    "            print(sample_output)\n",
    "\n",
    "            # `train` just changes the *mode* (train vs. eval), it doesn't *perform* the training.\n",
    "            model.train()\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs  = model(input_ids = b_input_ids,\n",
    "                             attention_mask = b_masks,\n",
    "                             labels = b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "       Training Loss  Valid. Loss Training Time Validation Time\nepoch                                                          \n1           4.789589     1.894683       1:50:03         0:02:39\n2           1.871892     1.804658       1:46:04         0:02:53\n3           1.786069     1.785056       1:37:31         0:02:40",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>4.789589</td>\n      <td>1.894683</td>\n      <td>1:50:03</td>\n      <td>0:02:39</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.871892</td>\n      <td>1.804658</td>\n      <td>1:46:04</td>\n      <td>0:02:53</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.786069</td>\n      <td>1.785056</td>\n      <td>1:37:31</td>\n      <td>0:02:40</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T22:06:03.382934100Z",
     "start_time": "2024-03-08T22:06:03.336026100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving & Loading Fine-Tuned Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model\n"
     ]
    },
    {
     "data": {
      "text/plain": "('./model\\\\tokenizer_config.json',\n './model\\\\special_tokens_map.json',\n './model\\\\vocab.json',\n './model\\\\merges.txt',\n './model\\\\added_tokens.json',\n './model\\\\tokenizer.json')"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Saving model to %s\" % model_save_path)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T01:26:09.294990200Z",
     "start_time": "2024-03-09T01:26:07.191726900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients: eggs, mushroom, butter, sugar, milk, cream, cornstarch, salt, brown sugar, flour, baking powder, baking soda, baking soda. Instructions: In a large saucepan, bring eggs to boil. Reduce heat and simmer until thickened. Remove from heat and stir in mushroom, butter, sugar, milk, cornstarch, salt, and brown sugar. Add the flour, baking powder and baking soda; stir until well blended. Add the baking soda and stir until completely dissolved. Add milk and cornstarch mixture to the mushroom mixture.  Stir well to combine. Divide mixture into 8 equal parts and bake at 350 degrees for 15 minutes. Let stand 5 minutes before serving. Makes 4-6 servings.\n"
     ]
    }
   ],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n",
    "# model.to(device)\n",
    "print(infer(\"eggs, mushroom, butter, sugar\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T14:09:09.981867200Z",
     "start_time": "2024-03-09T14:08:49.022337900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Ingredients: onion, garlic, chicken breast, fresh basil, butter, butter, brown sugar, salt. Instructions: Place chicken breasts in a slow cooker on low heat. Cook for 3 hours. Drain well. Pour 1/2 teaspoon of oil into a mixing bowl. Add onion and garlic. Stir and cook for another 3 hours or until translucent. Add chicken breast. Add fresh basil, butter and stir until well blended. Pour mixture into slow cooker. Cover and cook for 1 hour or until tender.'"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(\"onion, garlic, chicken breast\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T14:17:19.206923700Z",
     "start_time": "2024-03-09T14:17:05.347266900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients: avocado, lime juice, lime zest, lime juice, soy sauce, lime zest, soy sauce, lemon juice, fish sauce, lime, coconut, brown sugar, brown sugar, brown sugar, fish sauce, coconut. Instructions: Mix together avocado, lime juice, zest, lime juice, soy sauce, lime z zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy, lime juice, soy sauce, lime juice, soy sauce, lime juice, soy sauce, lime juice, soy sauce, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime juice, soy sauce, lime zest, lime\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"avocado, lime\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T22:06:35.542385500Z",
     "start_time": "2024-03-08T22:06:22.699748100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients: beef, salt, pepper, onions, garlic. Instructions: Cut the beef into 1-inch cubes and discard the fat. Melt the butter in a small saucepan, and add the onion. Stir-fry for 2 minutes. Add the garlic and saute until golden brown. Add the beef cubes and cook for 5 minutes. Pour the remaining butter in a heavy bottomed pan and bring to a boil. Stir occasionally, simmer for 10 minutes. Sprinkle the chopped parsley over the beef cubes, and cook for 3 minutes more, stirring occasionally. Return the beef to the saucepan and stir. Add the salt, pepper, onion, garlic and remaining butter to the beef cubes. Cook for 2 minutes more. Season with salt and pepper. Spoon the beef into a lasagna pan, and spread the sauce evenly over. Top with fresh parsley and tomato sauce. Refrigerate for 5 minutes. Cut into\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"beef, salt, pepper\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T14:30:06.050047600Z",
     "start_time": "2024-03-09T14:29:39.869279700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(10):\n",
    "    ingredients = val_dataset[i][2]\n",
    "    reference = val_dataset[i][3]\n",
    "    candidate = infer(ingredients)\n",
    "    scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "print(statistics.mean(scores))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
